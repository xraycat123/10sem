{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import json\n",
    "import jsonlines\n",
    "from spacy.strings import StringStore\n",
    "from spacy.tokens import Span\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings\n",
    "from typing import List\n",
    "from flair.embeddings import FlairEmbeddings\n",
    "import torch\n",
    "from flair.visual.training_curves import Plotter\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.training_utils import Metric\n",
    "import nerviz as nv\n",
    "import accuracy as ac\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.text import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0: 'text', 1:'ner'}\n",
    "data_folder = 'C:/Users/Martin/Desktop/Notebooks/speciale_ner/finaldataNER/'\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
    "                                                              train_file='train.txt',\n",
    "                                                              test_file='test.txt',\n",
    "                                                              dev_file='val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.obtain_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger: SequenceTagger = SequenceTagger.load_from_file('resources/taggers/example-1nertiw2/best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = [### SENTENCES should be here]\n",
    "\n",
    "# # predict NER tags\n",
    "# tagger.predict(sentence)\n",
    "dis_predictions = []\n",
    "for sen in sentences:   \n",
    "    sss = Sentence(sen,use_tokenizer=True)\n",
    "    aa = tagger.predict(sss)\n",
    "    \n",
    "    print(sss.to_tagged_string())\n",
    "    print(sss.get_spans('ner'))\n",
    "    dis_predictions.append((sss.to_tagged_string(),sss.get_spans('ner'),sss))\n",
    "    print(\"***\")\n",
    "\n",
    "\n",
    "\n",
    "# print sentence with predicted tags\n",
    "#print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this to run 4\n",
    "# 2019-06-04 01:14:46,229 MICRO_AVG: acc 0.8254 - f1-score 0.9043\n",
    "# 2019-06-04 01:14:46,230 MACRO_AVG: acc 0.9116 - f1-score 0.9515"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = tagger.predict(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search string:  atrial premature depolarizations \n",
    "# returned:  (0.6200701346711033, 'ectopic atrial beats')\n",
    "\n",
    "# ['atrial pacemaker artifact ', 'pacemaker capture ']\n",
    "# (['C0340914', 'C0340923'], [['234', '259'], ['241', '250', '268', '275']])\n",
    "# Sinus rhythm and underlying sinus rhythm without ventricular sensing of the atrial pacemaker. There is intermittent appearance of apparent atrial pacemaker artifact without capture. Clinical correlation is suggested. TRACING #2  \n",
    "\n",
    "# ['Sinus tachycardia ', 'chronic pulmonary disease ', 'sinus tachycardia ']\n",
    "# (['C0039239', 'C0746102', 'C0039239'], [['113', '130'], ['255', '280'], ['325', '342']])\n",
    "# Baseline artifact. Sinus tachycardia. Indeterminate QRS axis. Low limb leads voltage - is nonspecific. Prominent inferolateral Q waves - are nondiagnostic. Consider chronic pulmonary disease. Since previous tracing of [**2011-09-05**], sinus tachycardia present  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "summer = 0\n",
    "for sent in corpus.test:\n",
    "    k: Span = sent.get_spans('ner')\n",
    "    for i in k:\n",
    "        #print(len(i.tokens))\n",
    "        summer +=len(i.tokens)\n",
    "        counter += 1\n",
    "        \n",
    "print(summer/counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted: Sentence = corpus.test[90]\n",
    "vis2 = nv.NerVisualizer(predicted)\n",
    "vis2.plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = ac.Accuracy(tagger, corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues, preds = acc.get_preds_trues(mes_err = True,print_table=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dim(a):\n",
    "    if not type(a) == list:\n",
    "        return []\n",
    "    return [len(a)] + dim(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc.error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(acc.error_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def get_wrongs():\n",
    "    sorted_x ={}\n",
    "    wordza =  {}\n",
    "    for doc in acc.error_table:\n",
    "        for line in doc:\n",
    "            if line[0] != line[1]:\n",
    "               # print(line[0], line[1])\n",
    "                if line[2] in wordza:\n",
    "                    wordza[line[2]] = 1 +  wordza[line[2]]\n",
    "                else:\n",
    "                    wordza[line[2]] = 1\n",
    "\n",
    "\n",
    "    return sorted(wordza.items(), key=lambda kv: kv[1],reverse=True)\n",
    "    \n",
    "        #print(line)\n",
    "sortedx = get_wrongs()\n",
    "def plot_n_wrongs(n: int ,sortedx: dict):\n",
    "\n",
    "    topti = sortedx[:n]\n",
    "    x = np.arange(n)\n",
    "    x_names = list(map(lambda x: x[0],topti))\n",
    "    yval = list(map(lambda x: x[1],topti))\n",
    "    yval_norm = [i/sum(yval) for i in yval]\n",
    "    plt.bar(x,yval_norm)\n",
    "    ax = plt.xticks(x, x_names,rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('barplotmestforkert.svg')\n",
    "plot_n_wrongs(10,sortedx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.get_acc('POS_DIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trues, preds = acc.get_preds_trues(mes_err = False)\n",
    "acc.get_acc('POS_DIS')\n",
    "acc.plot_confusion_matrix(normalize=True,cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trues, preds = acc.get_preds_trues(mes_err = False)\n",
    "# acc.get_acc('POSDIS')\n",
    "# acc.plot_confusion_matrix(normalize=False,cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev score 3x cross validation  precision: 0.8889 - recall: 0.9182 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2019-05-13 19:28:29,834 POSDIS     tp: 696 - fp: 87 - fn: 62 - tn: 696 - precision: 0.8889 - recall: 0.9182 - accuracy: 0.8237 - f1-score: 0.9033"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Negation Detektion Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0: 'text', 1:'ner'}\n",
    "data_folder = 'C:/Users/Martin/Desktop/Notebooks/speciale_ner/datenegreal/'\n",
    "corpus_context: TaggedCorpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
    "                                                              train_file='train.txt',\n",
    "                                                              test_file='test.txt',\n",
    "                                                              dev_file='val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def add_row(dataset, text, target,realword):\n",
    "    #df2 = pd.DataFrame([[text, target]], columns = ['text','target'])\n",
    "    #dataset['text','target'] =[text, target]\n",
    "    dataset = dataset.append({'text':text, 'target':target, 'realword':realword},ignore_index=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def sentence_2_df(df,sentence):\n",
    "    sent = sentence\n",
    "   # print(len(sentence))\n",
    "    spans_in_txt = []\n",
    "    for span in sent.get_spans('ner'): \n",
    "        idx = [token.idx for token in span.tokens]    \n",
    "        spans_in_txt.append((idx,span.tag))\n",
    "        #print(span.tag)\n",
    "\n",
    "    marker = \"\"\n",
    "    #print(spans_in_txt)\n",
    "   # return \n",
    "    #pdb.set_trace()\n",
    "    index = len(sent.get_spans('ner'))\n",
    "   # print(index) \n",
    "  #  print(spans_in_txt,sent.get_spans('ner'))\n",
    "   # return None\n",
    "    \n",
    "    \n",
    "\n",
    "    for idx in range(index):\n",
    "        marker = \"\"\n",
    "        for i in range(1,len(sent)+1): #ta tag et ord ad gangen\n",
    "\n",
    "            if spans_in_txt[idx][0][0]==i:\n",
    "                #print(spans_in_txt[idx][0][0],i)\n",
    "                #break\n",
    "                marker += \"xxstart \"\n",
    "            elif spans_in_txt[idx][0][-1]==i-1:\n",
    "                marker += \"xxslut \"\n",
    "            val = sent.get_token(i)\n",
    "            marker += val.text + ' ' \n",
    "            #print(val.text)\n",
    "            \n",
    "        #print(marker, \"\\n\",spans_in_txt[idx][1])\n",
    "        if spans_in_txt[idx][1] != \"\":\n",
    "            #print(marker)\n",
    "            \n",
    "            #replaced = re.sub('(?<=xxstart)(.*)(?=xxslut)', 'xxdisease ', marker)\n",
    "            rword = re.findall('(?<=xxstart)(.*)(?=xxslut)',marker)\n",
    "            replaced = re.sub('xxstart(.*)xxslut', 'xxstart', marker)\n",
    "            \n",
    "            df = add_row(df, replaced, spans_in_txt[idx][1],rword)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for test in corpus_context.train:\n",
    "    df = sentence_2_df(df, test)\n",
    "\n",
    "for test in corpus_context.test:\n",
    "    df = sentence_2_df(df, test)\n",
    "    \n",
    "for test in corpus_context.dev:\n",
    "    df = sentence_2_df(df, test)\n",
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = pd.DataFrame()\n",
    "for i in range(len(data)):\n",
    "    #print(data.iloc[0].text, \" \\n\")\n",
    "    #p = \"Good morning Dr. Adams. The patient is waiting for you in room number 3.\"\n",
    "    mus = tokenize.sent_tokenize(data.iloc[i].text)\n",
    "    for sent in mus:\n",
    "        if 'xxstart' in sent:\n",
    "            df2 = df2.append({'text':sent, 'target':data.iloc[i].target},ignore_index=True)\n",
    "   #         print(sent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/Martin/Desktop/Notebooks/speciale_ner'\n",
    "\n",
    "bs = 16\n",
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('fi.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_for_clas = pd.read_csv('fi.csv') #xxstart <--- the disease\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_for_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 er den der er test\n",
    "# 17 er validering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data_clas = (TextList.from_df(data_for_clas, vocab=data_lm.vocab,cols='text')\n",
    "             #grab all the text files in path\n",
    "             .split_by_rand_pct(0.30,seed=17)\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_df(classes=['POS_DIS', 'NEG_DIS','UN_DIS'],cols='target')\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5,pretrained=True) # metrics=[F1()]\n",
    "learn.load_encoder('fine_tuned_enc_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.train_ds[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))\n",
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))\n",
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.91 procent uden prætræning overhovedet\n",
    "# 0.93 med prætræning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, slice(1e-7/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0\t0.181281\t0.208837\t0.936073\t00:56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('sent_classifier') # \n",
    "#learn.save('fit_classifier') # \n",
    "#learn.save('sent_classifier') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = learn.get_preds(ds_type=DatasetType.Valid, ordered=True)\n",
    "\n",
    "predictions = np.argmax(preds, axis = 1)\n",
    "pd.crosstab(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = learn.get_preds(ds_type=DatasetType.Valid, ordered=True)\n",
    "\n",
    "predictions = np.argmax(preds, axis = 1)\n",
    "pd.crosstab(targets,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.argmax(preds.numpy(),axis=1)\n",
    "targets_y = targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "# class_names = iris.target_names\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# # Run classifier, using a model that is too regularized (C too low) to see\n",
    "# # the impact on the results\n",
    "# classifier = svm.SVC(kernel='linear', C=0.01)\n",
    "# y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normaliseret confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "   # cm *=100.0\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Sand label',\n",
    "           xlabel='Prædikteret label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(6, 6)\n",
    "    fig.savefig('umlfittest2.svg')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "#np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "#plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                    #  title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "class_names =['Bekræftet','Negeret','Ubekræftet']\n",
    "plot_confusion_matrix(targets_y, pred_y, classes=class_names, normalize=False,\n",
    "                      title='Confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.argmax(preds.numpy(),axis=1)\n",
    "targets_y = targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['POS_DIS', 'NEG_DIS', 'UN_DIS']\n",
    "print(classification_report(targets_y, pred_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(targets_y, pred_y, average='micro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = np.argmax(preds.numpy(),axis=1)\n",
    "targets_y = targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y = label_binarize(targets_y, classes=[0, 1, 2])\n",
    "prediz = preds.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn to predict each class against the other\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y[:, i], prediz[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y.ravel(), prediz.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y.shape[1]\n",
    "# Plot linewidth.\n",
    "lw = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure(1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Falsk positiv Rate')\n",
    "plt.ylabel('Sand Positive Rate')\n",
    "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Zoom in view of the upper left corner.\n",
    "plt.figure(2)\n",
    "plt.xlim(0, 0.2)\n",
    "plt.ylim(0.6, 1.1)\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Zoom')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows=1000\n",
    "dataset = pd.DataFrame({'Preds':predictions.numpy(),'True':targets.numpy()})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"normal sinus rhythm . prolonged qt interval baseline artifact since previous tracing baseline artifact present and xxstart has changed to cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"sinus rhythm . short p - r interval of 0.10 seconds . qs deflections in leads vi - v2 . t wave inversions in leads i , ii , avl and v5-v6 which are non - specific . st segment elevation in leads vi - v2 shows that xxstart is present of undetermined age . compared to the previous tracing of _ date late ventricular premature beats were present immediately following p waves . anteroseptal myocardial infarction pattern of undetermined age was previously present . the p - r interval was normal rather than short . \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_ci = TextClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_ci.show_top_losses(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "^[ \\t]*$\\r?\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??txt_ci.show_intrinsic_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_classifier') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??txt_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt_ci.html_intrinsic_attention(###INSERT ECG NOTE HER,cmap=cm.Purples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def show_mistakes():\n",
    "    wrongs = dataset.loc[dataset.Preds!=dataset[\"True\"]]\n",
    "    \n",
    "    for i in wrongs.index:  \n",
    "        print(learn.data.valid_ds[i][1], \" Predicted: \", str(dataset.Preds[i]))\n",
    "        txt_ci = TextClassificationInterpretation.from_learner(learn)\n",
    "        test_text = learn.data.valid_ds[i][0]\n",
    "        txt_ci.show_intrinsic_attention(test_text,cmap=cm.Purples)\n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        #print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinus rhythm with atrial premature beats . inferior wall myocardial infarction . consider apical - xxstart . consider posterior myocardial infarction . st - t wave abnormalities . since the previous tracing of _ date atrial premature beats are new . tracing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mistakes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prnt_str_iob(k=2):\n",
    "    sds = ''\n",
    "    for i in range(len(acc.error_table[k])): \n",
    "        #print(acc.error_table[k][i][2])\n",
    "        sds += acc.error_table[k][i][2] + \" \"\n",
    "    #print(sds)\n",
    "    return sds, acc.error_table[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc.error_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sss = Sentence(\"normal sinus rhytm . premature ventricular contractions . left ventricular hypertropy . inferior st elevation - myocardial injury is suspected . since last ecg , t wave changes laterally\",use_tokenizer=True)\n",
    "#sss = Sentence(\"sinus tachycardia . left atrial abnormality . left bundle - branch block . compared to the previous tracing of _date atrial arrhythmias are no longer recorded . otherwise , not change\",use_tokenizer=True)\n",
    "sss = Sentence(\"sinus rhytm with a - v conduction prolongation . left anterior fascicular block . probable old extensive anterior and inferior myocardial infarction with very low qrs voltage .\",use_tokenizer=True)\n",
    "aa = tagger.predict(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = sss\n",
    "a = 0\n",
    "for i in range(1,len(sentence.tokens)):\n",
    "    a += sentence.get_token(i).tags['ner'].score\n",
    "    print(sentence.get_token(i), \" \" ,sentence.get_token(i).tags['ner'].value, \" \", sentence.get_token(i).tags['ner'].score ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??vis2.palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = Sentence(sss2,use_tokenizer=True)\n",
    "aa = tagger.predict(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#predicted: Sentence = corpus.test[90]\n",
    "#zoom 125\n",
    "vis2 = nv.NerVisualizer(sss)\n",
    "vis2.palette=['lightpink','green']\n",
    "#vis2.plt2(sss2,[[5,7]])\n",
    "vis2.plt()\n",
    "#vis2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from Authentication import *\n",
    "import requests\n",
    "import json\n",
    "import argparse\n",
    "#searchSCTresults=searchTerms.metode(\"sinus tachycardia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import searchTerms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchSCTresults=searchTerms.metode(\"sinus tachycardia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchSCTresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent: Sentence = dis_predictions[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hej: Span = sent.get_spans('ner')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sent.get_spans('ner')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation will be carried out according to the following F-scores:\n",
    "\n",
    "Strict F-score: a predicted mention is considered a true positive if (i) its predicted span is exactly the same as for the gold-standard mention; and (ii) the predicted CUI is correct.  The predicted disorder is considered a false positive if the span is incorrect or the CUI is incorrect.\n",
    "\n",
    "Relaxed F-score: a predicted mention is a true positive if (i) there is any word overlap between the predicted mention span and the gold-standard span (both in the case of contiguous and discontiguous spans); and (ii) the predicted CUI is correct. The predicted mention is a false positive if the span shares no words with the gold-standard span or the CUI is incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksim(sentence: str, vocab: list, thres: float = 0.6):\n",
    "    model = FT_gensim.load('xx.dd')\n",
    "    if sentence == None: return\n",
    "    candidates = []\n",
    "    for term in vocab:\n",
    "        try:\n",
    "            cos_sim = model.wv.similarity(sentence,term)\n",
    "        except:\n",
    "            cos_sim = 0.0\n",
    "            print(\"no ngrams !\")\n",
    "        \n",
    "        if cos_sim>thres: candidates.append((cos_sim,term))\n",
    "    \n",
    "    sorted_terms = sorted(candidates, key=lambda tup: tup[0])\n",
    "    if len(sorted_terms)>0: return sorted_terms[-1]\n",
    "    else:                 return None\n",
    "    \n",
    "checksim('atrioventricular conduction delay',cand)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('myocardial infarction', 'prior anteroseptal myocardial infarction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('prior anteroseptal myocardial infarction','myocardial infarction'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.morphology.lemmatizer(u'endosomes', 'noun', morphology={'number': 'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from srcalg.sieve import sieve\n",
    "def sieve_algo(text):\n",
    "    a = exact_macth(text)\n",
    "    if a == 'NONE':\n",
    "        a = hyphenation(text)\n",
    "    if a == 'NONE':\n",
    "        a = abreviation(text)\n",
    "    if a == 'NONE':\n",
    "        a = norepl(text)\n",
    "    if a == 'NONE':\n",
    "        a = partialmatch(text)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrlist = [## insert arblist here\n",
    "          \n",
    "          ]\n",
    "number_converter = [('1st','first'),('2nd','second'),('3rd','third'),('4rd','fourth'),('5th','fifth'),('6th','sixth'),\n",
    "                   ('7th','seventh'),('8th','eighth'),('9th','ninth'),('10th')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand = ['old inferior myocardial infarction','old anterior myocardial infarction','anterior myocardial infarction','infarction','infarct','ectopic atrial beats','atrial fibrillation','sinus tachycardia','first degree atrioventricular block','atrioventricular conduction disorder','Ventricular ectopic beats','inferior myocardial infarction','acute anterolateral myocardial infarction','myocardial infarction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_disorder = \"1st degree a-v block\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in dis_predictions:\n",
    "    doc: Sentence = i[2]\n",
    "   # print(span)\n",
    "    print(doc)\n",
    "    for span in doc.get_spans('ner'):\n",
    "        searchSCTresults=searchTerms.metode(span.text)\n",
    "        if searchSCTresults[0] == 'NONE':\n",
    "            \n",
    "            #print(\"Vector search \",checksim(span.text,cand))\n",
    "            #print(\"vector match\")\n",
    "            searchSCTresults = checksim(span.text,cand) # partial match with wordvectors\n",
    "            if searchSCTresults != None:\n",
    "                searchSCTresults = searchSCTresults\n",
    "            \n",
    "            \n",
    "        \n",
    "     \n",
    "        print(\"Search string: \",span.text, \"\\nreturned: \" , searchSCTresults)\n",
    "    print(\"*********************\")\n",
    "    print(\"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    #print(i[1])\n",
    "    #input(\"videre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 1 +1 +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Search string:  atrial premature depolarizations \n",
    "returned:  (0.6200701346711033, 'ectopic atrial beats')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Search string:  atrial premature depolarizations \n",
    "returned:  (0.6200701346711033, 'ectopic atrial beats')\n",
    "\n",
    "['atrial pacemaker artifact ', 'pacemaker capture ']\n",
    "(['C0340914', 'C0340923'], [['234', '259'], ['241', '250', '268', '275']])\n",
    "Sinus rhythm and underlying sinus rhythm without ventricular sensing of the atrial pacemaker. There is intermittent appearance of apparent atrial pacemaker artifact without capture. Clinical correlation is suggested. TRACING #2  \n",
    "\n",
    "['Sinus tachycardia ', 'chronic pulmonary disease ', 'sinus tachycardia ']\n",
    "(['C0039239', 'C0746102', 'C0039239'], [['113', '130'], ['255', '280'], ['325', '342']])\n",
    "Baseline artifact. Sinus tachycardia. Indeterminate QRS axis. Low limb leads voltage - is nonspecific. Prominent inferolateral Q waves - are nondiagnostic. Consider chronic pulmonary disease. Since previous tracing of [**2011-09-05**], sinus tachycardia present  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "junctional rhythm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(tp,fn,fp)-> tuple:\n",
    "    prec = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f = 2*(prec*recall/(prec+recall))\n",
    "    return f, prec, recall\n",
    "\n",
    "calculate_f1(104,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchSCTresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText as FT_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FT_gensim.load('xx.dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = datapath('dataset_lower_vec.cor')\n",
    "\n",
    "#model_gensim = FT_gensim(size=50)\n",
    "#model_gensim.build_vocab(corpus_file=corpus_file)\n",
    "model_gensim.load('ftmodel.dd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.wv.most_similar(positive=['1st'],topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('ischemia', 'sinus bradycardia')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('prior anterior myocardial infarction','sinus bradycardia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('prior anterior myocardial infarction','old anterior myocardial infarction'))\n",
    "print(model.wv.similarity('prior anterior myocardial infarction','anterior myocardial infarction'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('first degree atrioventricular block','1st degree a-v block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('vent. arrhyt.','sinus bradycardia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('sinus tachycardia','sinus bradycardia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('sin bradycardia','sinus bradycardia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('s. bra.','sinus bradycardia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.wv.similarity('atrial ectopy','ectopic atrial beats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.wv.similarity('atrial ectopy','atrial fibrillation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"prior anteroseptal myocardial infarction\", \"myocardial infarction\"]\n",
    "process.extract(\"sinus bradycardia\", choices, limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [\"vent. arrhyt.\", \"sinus tachycardia\", \"bradyarrhythmia\", \"sin bradycardia\"]\n",
    "process.extract(\"sinus bradycardia\", choices, limit=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('prior anteroseptal myocardial infarction','myocardial infarction'))\n",
    "prior anteroseptal myocardial infarction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labella.scale import LinearScale\n",
    "from labella.timeline import TimelineTex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'events.txt', parse_dates=True, index_col=0)\n",
    "ax = GenerateTimeLine(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/gantt_example.csv')\n",
    "\n",
    "fig = ff.create_gantt(df, colors=['#333F44', '#93e4c1'], index_col='Complete', show_colorbar=True,\n",
    "                      bar_width=0.2, showgrid_x=True, showgrid_y=True)\n",
    "py.iplot(fig, filename='gantt-use-a-pandas-dataframe', world_readable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline shizzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_for_clas.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://etav.github.io/projects/spam_message_classifier_naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df.target.map({'POS_DIS':0, 'UN_DIS':1, 'NEG_DIS':2})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string #punctuation\n",
    "import pprint\n",
    "from collections import Counter #frequencies\n",
    "\n",
    "#Bag of Words from scratch\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "\n",
    "lower_case_documents = []\n",
    "\n",
    "for i in documents:\n",
    "    lower_case_documents.append(i.lower())\n",
    "print(\"lower case:\", lower_case_documents)\n",
    "\n",
    "# Remove punctuation.\n",
    "sans_punctuation_documents = []\n",
    "\n",
    "for i in lower_case_documents:\n",
    "    sans_punctuation_documents = [\"\".join( j for j in i if j not in string.punctuation) for i in  lower_case_documents]\n",
    "print(\"no punctuation:\", (sans_punctuation_documents))\n",
    "\n",
    "#Break each word\n",
    "preprocessed_documents = []\n",
    "\n",
    "for i in sans_punctuation_documents:\n",
    "    preprocessed_documents.append(i.split(' ')) #split on space\n",
    "print(\"break words:\", (preprocessed_documents))\n",
    "\n",
    "#Count frequency of words using counter\n",
    "frequency_list = []\n",
    "\n",
    "for i in preprocessed_documents:\n",
    "    frequency_counts = Counter(i)\n",
    "    frequency_list.append(frequency_counts)\n",
    "print(\"tokenized counts:\", pprint.pprint(frequency_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer() #set the variable\n",
    "\n",
    "count_vector.fit(documents) #fit the function\n",
    "count_vector.get_feature_names() #get the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array,\n",
    "                                columns = count_vector.get_feature_names()\n",
    "                               )\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "\n",
    "df['tokenized_sents'] = df.apply(lambda row: ' '.join(i for i in nltk.word_tokenize(row['text'])), axis=1)\n",
    "#df['tokenized_sents2'] = df.apply(lambda row: ' '.join(map(str, row)), axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized_sents'],\n",
    "                                                    df['target'],\n",
    "                                                    test_size = 0.3)\n",
    "\n",
    "print(\"Our original set contains\", df.shape[0], \"observations\")\n",
    "print(\"Our training set contains\", X_train.shape[0], \"observations\")\n",
    "print(\"Our testing set contains\", X_test.shape[0], \"observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = count_vector.fit_transform(X_train)\n",
    "test = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#https://www.kaggle.com/aneeshc/text-classification-using-bag-of-word-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/aneeshc/text-classification-using-bag-of-word-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB())\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# naive_bayes = MultinomialNB() #call the method\n",
    "# naive_bayes.fit(train, y_train) #train the classifier on the training set\n",
    "# predictions = naive_bayes.predict(X_test) #predic using the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test) #predic using the model on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score,f1_score\n",
    "\n",
    "print(accuracy_score(y_test,predictions))\n",
    "print(f1_score(y_test,predictions,average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['POS_DIS', 'NEG_DIS', 'UN_DIS']\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['POS_DIS', 'NEG_DIS', 'UN_DIS']\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['POS_DIS', 'NEG_DIS', 'UN_DIS']\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['POS_DIS', 'NEG_DIS', 'UN_DIS']\n",
    "print(classification_report(targets_y, pred_y, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "for filename in [\"ADMISSIONS.csv.gz\"]:\n",
    "\n",
    "    with gzip.open(filename, 'rt') as f:\n",
    "         data = f.read()\n",
    "    with open(filename[:-3], 'wt') as f:\n",
    "         f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import odsc_utils\n",
    "\n",
    "df_adm_notes_clean = odsc_utils.load_clean_merge_dataset('ADMISSIONS.csv','NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_notes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_adm_notes_clean['OUTPUT_LABEL'] = (df_adm_notes_clean.DAYS_NEXT_ADMIT < 30).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive samples:', (df_adm_notes_clean.OUTPUT_LABEL == 1).sum())\n",
    "print('Number of negative samples:',  (df_adm_notes_clean.OUTPUT_LABEL == 0).sum())\n",
    "print('Total samples:', len(df_adm_notes_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the samples\n",
    "df_adm_notes_clean = df_adm_notes_clean.sample(n = len(df_adm_notes_clean), random_state = 42)\n",
    "df_adm_notes_clean = df_adm_notes_clean.reset_index(drop = True)\n",
    "\n",
    "# Save 30% of the data as validation and test data \n",
    "df_valid_test=df_adm_notes_clean.sample(frac=0.30,random_state=42)\n",
    "\n",
    "df_test = df_valid_test.sample(frac = 0.5, random_state = 42)\n",
    "df_valid = df_valid_test.drop(df_test.index)\n",
    "\n",
    "# use the rest of the data as training data\n",
    "df_train_all=df_adm_notes_clean.drop(df_valid_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test prevalence(n = %d):%.3f'%(len(df_test),df_test.OUTPUT_LABEL.sum()/ len(df_test)))\n",
    "print('Valid prevalence(n = %d):%.3f'%(len(df_valid),df_valid.OUTPUT_LABEL.sum()/ len(df_valid)))\n",
    "print('Train all prevalence(n = %d):%.3f'%(len(df_train_all), df_train_all.OUTPUT_LABEL.sum()/ len(df_train_all)))\n",
    "print('all samples (n = %d)'%len(df_adm_notes_clean))\n",
    "assert len(df_adm_notes_clean) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'\n",
    "\n",
    "print('Test samples ', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train prevalence (n = 4184): 0.5\n"
     ]
    }
   ],
   "source": [
    "# split the training data into positive and negative\n",
    "rows_pos = df_train_all.OUTPUT_LABEL == 1\n",
    "df_train_pos = df_train_all.loc[rows_pos]\n",
    "df_train_neg = df_train_all.loc[~rows_pos]\n",
    "\n",
    "n = np.min([len(df_train_pos),len(df_train_neg)])\n",
    "\n",
    "# merge the balanced data\n",
    "df_train = pd.concat([df_train_pos.sample(n = n, random_state = 13), \\\n",
    "                      df_train_neg.sample(n = n, random_state = 13)],axis = 0)\n",
    "\n",
    "# shuffle the order of training samples \n",
    "df_train = df_train.sample(n = len(df_train), random_state = 13).reset_index(drop = True)\n",
    "\n",
    "print('Train prevalence (n = %d):'%len(df_train), df_train.OUTPUT_LABEL.sum()/ len(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_pos = df_test.OUTPUT_LABEL == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    # This function preprocesses the text by filling not a number and replacing new lines ('\\n') and carriage returns ('\\r')\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT =df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT =df.TEXT.str.replace('\\r',' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the text to deal with known issues\n",
    "df_train = preprocess_text(df_train)\n",
    "df_valid = preprocess_text(df_valid)\n",
    "df_test = preprocess_text(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.TEXT = df_train[\"TEXT\"].map(lambda x: x.lower())\n",
    "df_test.TEXT = df_test[\"TEXT\"].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = 'path'\n",
    "\n",
    "bs = 16\n",
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = df_train['OUTPUT_LABEL'].value_counts().max()\n",
    "\n",
    "\n",
    "lst = [df_train]\n",
    "for class_index, group in df_train.groupby('OUTPUT_LABEL'):\n",
    "    lst.append(group.sample(max_size-len(group), replace=True))\n",
    "df_train = pd.concat(lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(df_train, cols='TEXT')\n",
    "             #grab all the text files in path\n",
    "             .split_by_rand_pct(0.15,seed=17)\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_df(cols='OUTPUT_LABEL',classes=[0,1 ])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mbsariyildiz/focal-loss.pytorch/blob/master/focalloss.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, gamma=2, alpha=1, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)                         # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))    # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From hackernoon\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduce = reduce\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = loss(inputs, targets.squeeze(1))\n",
    "       \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict('this is ehh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.3,pretrained=False) # metrics=[F1()]\n",
    "#learn.load_encoder('fine_tuned_enc_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('level2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.loss_func = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://forums.fast.ai/t/highly-imbalanced-data/37001/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.692435</td>\n",
       "      <td>0.782179</td>\n",
       "      <td>0.511962</td>\n",
       "      <td>17:50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))\n",
    "# learn.freeze_to(-2)\n",
    "# learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))\n",
    "# learn.freeze_to(-3)\n",
    "# learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))\n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('level1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-3), moms=(0.8,0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('level2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('level2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('level3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, targets = learn.get_preds(ds_type=DatasetType.Valid, ordered=True)\n",
    "pred_y = np.argmax(preds.numpy(),axis=1)\n",
    "targets_y = targets.numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in preds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in targets:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [0,1]\n",
    "#print(classification_report(targets_y, pred_y, target_names=target_names))\n",
    "f1_score(targets_y, pred_y, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0    1\n",
       "row_0         \n",
       "0      19  298\n",
       "1       8  302"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predictions = np.argmax(preds, axis = 1)\n",
    "pd.crosstab(targets_y,pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(targets, predictions.numpy(), average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preds[:,1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc_train = roc_auc_score(targets, preds[:,1])\n",
    "#auc_valid = roc_auc_score(y_valid, y_valid_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.566805739289712"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(targets, preds[:,1].numpy(), pos_label=1)\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[:,1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "646px",
    "left": "1487px",
    "right": "20px",
    "top": "122px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
